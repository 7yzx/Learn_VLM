{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5db10e79",
   "metadata": {},
   "source": [
    "## transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91885e47",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "  def __init__(self, d_model, n_heads, mlp_ratio =4):\n",
    "      super().__init__()\n",
    "\n",
    "      self.d_model = d_model\n",
    "      self.n_heads = n_heads\n",
    "\n",
    "      self.ln1 = nn.LayerNorm(d_model)\n",
    "\n",
    "      self.mha = MultiheadAttention(d_model, n_heads)\n",
    "\n",
    "      self.ln2 = nn.LayerNorm(d_model)\n",
    "\n",
    "      self.mlp = nn.Sequential(\n",
    "          nn.Linear(d_model, d_model*mlp_ratio),\n",
    "          nn.GELU(),\n",
    "          nn.Linear(d_model * mlp_ratio, d_model)\n",
    "      )\n",
    "\n",
    "  #For clip even though its a encoder model it requires mask ->to account for padded for max seq_length\n",
    "  def forward(self, x, mask = None):\n",
    "\n",
    "      x_n = self.mha(self.ln1(x), mask = mask)\n",
    "      x = x + self.mlp(self.ln2(x_n))\n",
    "\n",
    "      return x  # x.shape -->  [B,max_seq_len,d_model]\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "  def __init__(self, d_model, qkv_dim):\n",
    "    super().__init__()\n",
    "\n",
    "    self.qkv_dim = qkv_dim\n",
    "\n",
    "    self.query = nn.Linear(d_model, qkv_dim)\n",
    "    self.key = nn.Linear(d_model, qkv_dim)\n",
    "    self.value = nn.Linear(d_model, qkv_dim)\n",
    "\n",
    "  def forward(self, x, mask=None):\n",
    "    # x.shape --> [B, max_seq_len, d_model]\n",
    "    Q = self.query(x)\n",
    "    K = self.key(x)\n",
    "    V = self.value(x)\n",
    "\n",
    "    attention = Q @ K.transpose(-2, -1) #[B, max_seq_len, max_seq_len]\n",
    "    attention = attention / (self.qkv_dim ** 0.5)\n",
    "    # apply attention mask for padded sequence\n",
    "    if mask is not None:\n",
    "      mask = attention.masked_fill(\n",
    "          mask == 0, float(\"-inf\")\n",
    "      )# torch.tensor.masked_fill\n",
    "\n",
    "    attention = torch.softmax(attention , dim=-1) # (softmax(Q_K^T)/sqrt(d_k)).V\n",
    "\n",
    "    attention = attention @ V\n",
    "\n",
    "    return attention # Y_i\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "  def __init__(self, d_model, n_heads):\n",
    "    super().__init__()\n",
    "    # d_model --> embed dimension\n",
    "    # n_heads --> nums of heads\n",
    "    self.qkv_dim = d_model // n_heads\n",
    "\n",
    "    self.W_o = nn.Linear(d_model, d_model)\n",
    "    self.multi_head = nn.ModuleList(\n",
    "        [AttentionHead(d_model, self.qkv_dim) for _ in range(n_heads)]\n",
    "    )\n",
    "\n",
    "  def forward(self, x, mask=None):\n",
    "    # x.shape --> [B, max_seq, d_model]\n",
    "    # concatenates the outputs\n",
    "\n",
    "    out = torch.cat(\n",
    "        [head(x , mask=mask) for head in self.multi_head], dim=-1\n",
    "    ) # [ B, max_seq_len, d_model]\n",
    "\n",
    "    out = self.W_o(out) # [B, max_seq_len, d_model]\n",
    "\n",
    "    return out\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "  def __init__(self, d_model, max_seq_len):\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.max_seq_len = max_seq_len\n",
    "    pe = torch.zeros(max_seq_len, d_model)\n",
    "    position = torch.arange(0, max_seq_len).unsqueeze(1)\n",
    "    div_term = torch.exp(\n",
    "        torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model)\n",
    "    )\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x.shape --> [B, max_seq_len, d_model]\n",
    "    seq_len = x.size(1)\n",
    "    return x + self.pe[:, :seq_len]\n",
    "    # [B, max_seq_len, d_model] + [1, max_seq_len, d_model]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf36bafc",
   "metadata": {},
   "source": [
    "## vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a2ab86",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from .transformer import TransformerEncoder, PositionalEmbedding \n",
    "\n",
    "class VisionEncoder(nn.Module):\n",
    "  def __init__(self, d_model, img_size, patch_size,\n",
    "               n_channels, n_heads, n_layers, emb_dim):\n",
    "    super().__init__()\n",
    "    assert (\n",
    "        img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0\n",
    "    ), \"image dimensions should be divisible by patch dim\"\n",
    "    assert d_model % n_heads == 0, \"d_model should be divisible by n_heads\"\n",
    "\n",
    "    self.num_patches = (img_size[0] * img_size[1]) // (patch_size[0] * patch_size[1])\n",
    "    # max_seq length\n",
    "\n",
    "    self.max_seq_length = self.num_patches + 1\n",
    "\n",
    "    self.linear_proj = nn.Conv2d(\n",
    "        in_channels=n_channels,\n",
    "        out_channels=d_model,\n",
    "        kernel_size=patch_size,\n",
    "        stride=patch_size[0],\n",
    "    )\n",
    "\n",
    "    self.cls_token = nn.Parameter(torch.randn(1,1,d_model),requires_grad=True)\n",
    "\n",
    "    self.positional_embedding = PositionalEmbedding(d_model, self.max_seq_length)\n",
    "\n",
    "    self.transformer_encoder = nn.ModuleList(\n",
    "        [TransformerEncoder(d_model, n_heads) for _ in range(n_layers)]\n",
    "    )\n",
    "\n",
    "    self.projection = nn.Parameter(torch.randn(d_model, emb_dim))\n",
    "\n",
    "  def forward(self, x, mask=None):\n",
    "    x = self.linear_proj(x)\n",
    "    # [B,C,H,W] -> (B, d_model, patch_col_d_model, patch_row_d_model)\n",
    "    x = x.flatten(2).transpose(-2, -1)\n",
    "    # (B, d_model, Patch_col_d_model, Patch_row_height) --> Flatten (B, d_model, Patch) --> .transpose(-2,-1) (B, Patch, d_model)\n",
    "\n",
    "    x = torch.cat(\n",
    "        (self.cls_token.expand(x.shape[0], -1, -1), x), dim=1\n",
    "    )\n",
    "\n",
    "    x = self.positional_embedding(x)\n",
    "\n",
    "    for encoder_layer in self.transformer_encoder:\n",
    "      x = encoder_layer(x, mask)\n",
    "\n",
    "    x = x[:, 0, :]\n",
    "\n",
    "    if self.projection is not None:\n",
    "      x = x @ self.projection\n",
    "\n",
    "    x = x / torch.norm(x, dim=-1,keepdim=True)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991d704f",
   "metadata": {},
   "source": [
    "tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ea4298",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from .transformer import TransformerEncoder, PositionalEmbedding\n",
    "\n",
    "def tokenizer(text, encode=True, mask=None, max_seq_length=32):\n",
    "  if encode:\n",
    "    # CLIP 在输入文本的开头和结尾分别加上 [BOS] (Begin Of Sequence) 和 [EOS] (End Of Sequence) 标记。\n",
    "    # 这里用 chr(2) 代表 BOS，chr(3) 代表 EOS\n",
    "    out = chr(2) + text + chr(3)\n",
    "\n",
    "    # 代码确保所有输出长度一致为 max_seq_length。太长就切掉，太短就补 chr(0)。\n",
    "    if len(out) > max_seq_length:\n",
    "      out = out[:max_seq_length]\n",
    "\n",
    "    out = out + \"\".join(\n",
    "        [chr(0) for _ in range(max_seq_length - len(out))]\n",
    "    )\n",
    "\n",
    "    out = torch.IntTensor(list(out.encode(\"utf-8\")))\n",
    "\n",
    "    mask = torch.ones(len(out.nonzero()))\n",
    "\n",
    "    if len(mask) < max_seq_length:\n",
    "      mask = torch.cat((mask, torch.zeros(max_seq_length - len(mask)))).type(torch.IntTensor)\n",
    "    else:\n",
    "      mask = mask.type(torch.IntTensor)\n",
    "  else:\n",
    "    out = [chr(x) for x in text[1: len(mask.nonzero()) - 1]]\n",
    "    out = \"\".join(out)\n",
    "    mask = None\n",
    "  \n",
    "  return out, mask\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "  def __init__(self, vocab_size, d_model, max_seq_length, n_layers,\n",
    "               n_heads, emb_dim):\n",
    "    super().__init__()\n",
    "    self.max_seq_length = max_seq_length\n",
    "    self.embed = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    self.positional_embedding = PositionalEmbedding(d_model, max_seq_length)\n",
    "\n",
    "    self.transformer_encoder = nn.ModuleList(\n",
    "        [TransformerEncoder(d_model, n_heads) for _ in range(n_layers)]\n",
    "    )\n",
    "\n",
    "    self.projection = nn.Parameter(torch.randn(d_model, emb_dim))\n",
    "\n",
    "  def forward(self, text, mask=None):\n",
    "    x = self.embed(text)\n",
    "    x = self.positional_embedding(x)\n",
    "\n",
    "    for encoder_layer in self.transformer_encoder:\n",
    "      x = encoder_layer(x, mask=mask) # [B, max_seq_length, d_model]\n",
    "    \n",
    "\n",
    "    # Transformer 输出的 x 是一个序列，包含每个单词的特征（比如 [SOT, \"一只\", \"狗\", EOT, Pad, Pad...]）。\n",
    "    # 取出每句话最后一个有效字符（即 EOS 标记）对应的向量。python 高级索引 给的是两个列表进行索引\n",
    "\n",
    "    x = x[\n",
    "        torch.arange(text.shape[0]), \n",
    "        torch.sub(torch.sum(mask[:,0], dim=1),1)\n",
    "          ]\n",
    "    \n",
    "    # 投影到多模态公共空间。\n",
    "    if self.projection is not None:\n",
    "      x = x @ self.projection\n",
    "    \n",
    "    # L2 归一化\n",
    "    x = x / torch.norm(x, dim=-1, keepdim=True)\n",
    "\n",
    "    return x\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
